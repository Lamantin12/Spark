{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea85108-2185-4597-b0c4-923cd987e13a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Chapter 9: Building Reliable Data Lakes with Delta Lake and Apache Spark™\n",
    "\n",
    "Delta Lake: An open-source storage format that brings ACID transactions to Apache Spark™ and big data workloads.\n",
    "\n",
    "<img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" width=300/>\n",
    "\n",
    "\n",
    "* **Open format**: Stored as Parquet format in blob storage.\n",
    "* **ACID Transactions**: Ensures data integrity and read consistency with complex, concurrent data pipelines.\n",
    "* **Schema Enforcement and Evolution**: Ensures data cleanliness by blocking writes with unexpected.\n",
    "* **Audit History**: History of all the operations that happened in the table.\n",
    "* **Time Travel**: Query previous versions of the table by time or version number.\n",
    "* **Deletes and upserts**: Supports deleting and upserting into tables with programmatic APIs.\n",
    "* **Scalable Metadata management**: Able to handle millions of files are scaling the metadata operations with Spark.\n",
    "* **Unified Batch and Streaming Source and Sink**: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box. \n",
    "\n",
    "### Source:\n",
    "This notebook is a modified version of the [SAIS EU 2019 Delta Lake Tutorial](https://github.com/delta-io/delta/tree/master/examples/tutorials/saiseu19). The data used is a modified version of the public data from [Lending Club](https://www.kaggle.com/wendykan/lending-club-loan-data). It includes all funded loans from 2012 to 2017. Each loan includes applicant information provided by the applicant as well as the current loan status (Current, Late, Fully Paid, etc.) and latest payment information. For a full view of the data please view the data dictionary available [here](https://resources.lendingclub.com/LCDataDictionary.xlsx)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6599193e-6e43-4973-a7b0-422ccb13089d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Loading data in Delta Lake table\n",
    "\n",
    "First let’s, read this data and save it as a Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0697eb1-ad92-43b3-9dd2-8b91353d95a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined view 'loans_delta'\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"set spark.sql.shuffle.partitions = 1\")\n",
    "\n",
    "sourcePath = \"/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n",
    "\n",
    "# Configure Delta Lake Path\n",
    "deltaPath = \"/tmp/loans_delta\"\n",
    "\n",
    "# Remove folder if it exists\n",
    "dbutils.fs.rm(deltaPath, recurse=True)\n",
    "\n",
    "# Create the Delta table with the same loans data\n",
    "(spark.read.format(\"parquet\").load(sourcePath) \n",
    "  .write.format(\"delta\").save(deltaPath))\n",
    "\n",
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\n",
    "print(\"Defined view 'loans_delta'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7feae2bf-73b5-4b6e-ae79-93d1af41d01e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76cefec5-68ae-4484-b5ad-d9cfb4a99083",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   14705|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f46fbb77-f868-4860-a6a8-50d0db347de9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+\n|loan_id|funded_amnt|paid_amnt|addr_state|\n+-------+-----------+---------+----------+\n|      0|       1000|   182.22|        CA|\n|      1|       1000|   361.19|        WA|\n|      2|       1000|   176.26|        TX|\n|      3|       1000|   1000.0|        OK|\n|      4|       1000|   249.98|        PA|\n+-------+-----------+---------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM loans_delta LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec1cd1cf-d13e-4565-a06b-6b85bb1f79b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Loading data streams into Delta Lake table\n",
    "\n",
    "We will generate a stream of data from with randomly generated loan ids and amounts. \n",
    "In addition, we are going to define a few useful utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c575813-80d5-4ef0-af23-8f9d46d76729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "def random_checkpoint_dir(): \n",
    "  return \"/tmp/chkpt/%s\" % str(random.randint(0, 10000))\n",
    "\n",
    "\n",
    "# User-defined function to generate random state\n",
    "\n",
    "states = [\"CA\", \"TX\", \"NY\", \"WA\"]\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def random_state():\n",
    "  return str(random.choice(states))\n",
    "\n",
    "\n",
    "# Function to start a streaming query with a stream of randomly generated data and append to the parquet table\n",
    "def generate_and_append_data_stream():\n",
    "\n",
    "  newLoanStreamDF = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \n",
    "    .withColumn(\"loan_id\", 10000 + col(\"value\")) \n",
    "    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \n",
    "    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \n",
    "    .withColumn(\"addr_state\", random_state())\n",
    "    .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\"))\n",
    "    \n",
    "  checkpointDir = random_checkpoint_dir()\n",
    "\n",
    "  streamingQuery = (newLoanStreamDF.writeStream \n",
    "    .format(\"delta\") \n",
    "    .option(\"checkpointLocation\", random_checkpoint_dir()) \n",
    "    .trigger(processingTime = \"10 seconds\") \n",
    "    .start(deltaPath))\n",
    "\n",
    "  return streamingQuery\n",
    "\n",
    "# Function to stop all streaming queries \n",
    "def stop_all_streams():\n",
    "  # Stop all the streams\n",
    "  print(\"Stopping all streams\")\n",
    "  for s in spark.streams.active:\n",
    "    s.stop()\n",
    "  print(\"Stopped all streams\")\n",
    "  print(\"Deleting checkpoints\")  \n",
    "  dbutils.fs.rm(\"/tmp/chkpt/\", True)\n",
    "  print(\"Deleted checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fffdb864-3ee3-4b90-ad34-cee01284be14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streamingQuery = generate_and_append_data_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d389963b-7f44-4891-a009-85d166911a36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can see that the streaming query is adding data to the table by counting the number of records in the table. Run the following cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7fef689-1f35-4ad4-ad95-6c837fee1381",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|   15645|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM loans_delta\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca9d4145-54a6-4ef8-8a2f-eb1975521c55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Remember to stop all the streaming queries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43d45804-03c4-4425-886b-168bf12a09ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping all streams\nStopped all streams\nDeleting checkpoints\nDeleted checkpoints\n"
     ]
    }
   ],
   "source": [
    "stop_all_streams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b7df33-e310-4519-9533-408a5c3b4cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Enforcing schema on write to prevent data corruption\n",
    "\n",
    " Let’s test this by trying to write some data with an additional column `closed` that signifies whether the loan has been terminated. Note that this column does not exist in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b769996a-9405-4805-a78a-e7c908a8ad10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"closed\"]\n",
    "\n",
    "items = [\n",
    "  (1111111, 1000, 1000.0, \"TX\", True), \n",
    "  (2222222, 2000, 0.0, \"CA\", False)\n",
    "]\n",
    "\n",
    "loanUpdates = (spark\n",
    "                .createDataFrame(items, cols)\n",
    "                .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a013e176-697d-4db0-aefa-6295e8d4396b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-884566190556022>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Uncomment the line below and it will error\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mloanUpdates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeltaPath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: 8a736b01-b182-491c-be2e-57fbfe6d90b9).\n",
       "To enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n",
       "'.option(\"mergeSchema\", \"true\")'.\n",
       "For other operations, set the session configuration\n",
       "spark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\n",
       "specific to the operation for details.\n",
       "\n",
       "Table schema:\n",
       "root\n",
       "-- loan_id: long (nullable = true)\n",
       "-- funded_amnt: integer (nullable = true)\n",
       "-- paid_amnt: double (nullable = true)\n",
       "-- addr_state: string (nullable = true)\n",
       "\n",
       "\n",
       "Data schema:\n",
       "root\n",
       "-- loan_id: long (nullable = true)\n",
       "-- funded_amnt: integer (nullable = true)\n",
       "-- paid_amnt: double (nullable = true)\n",
       "-- addr_state: string (nullable = true)\n",
       "-- closed: boolean (nullable = true)\n",
       "\n",
       "         "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-884566190556022>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Uncomment the line below and it will error\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mloanUpdates\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mappend\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdeltaPath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: A schema mismatch detected when writing to the Delta table (Table ID: 8a736b01-b182-491c-be2e-57fbfe6d90b9).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: A schema mismatch detected when writing to the Delta table (Table ID: 8a736b01-b182-491c-be2e-57fbfe6d90b9).\nTo enable schema migration using DataFrameWriter or DataStreamWriter, please set:\n'.option(\"mergeSchema\", \"true\")'.\nFor other operations, set the session configuration\nspark.databricks.delta.schema.autoMerge.enabled to \"true\". See the documentation\nspecific to the operation for details.\n\nTable schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n\n\nData schema:\nroot\n-- loan_id: long (nullable = true)\n-- funded_amnt: integer (nullable = true)\n-- paid_amnt: double (nullable = true)\n-- addr_state: string (nullable = true)\n-- closed: boolean (nullable = true)\n\n         ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Uncomment the line below and it will error\n",
    "loanUpdates.write.format(\"delta\").mode(\"append\").save(deltaPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4ba0d1e-c1e9-4a77-9304-661ad080f28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Evolving schema to accommodate changing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3803db67-a578-4cc7-9995-55375c5e1343",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(loanUpdates.write.format(\"delta\").mode(\"append\")\n",
    "  .option(\"mergeSchema\", \"true\")\n",
    "  .save(deltaPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3442896-6d00-475f-ac66-f1ee7cb8a52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's query the table once again to see the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be3f6d5-1cb6-4c07-8c0a-244f4763567e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|1111111|       1000|   1000.0|        TX|  true|\n+-------+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).filter(\"loan_id = 1111111\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18b5fde-c189-4527-a0de-e5553bc6b909",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For existing rows are read, the value of the new column is considered as NULL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e05c3d1-08aa-4bcb-9dd5-50d3a22d2b95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+----------+------+\n|loan_id|funded_amnt|         paid_amnt|addr_state|closed|\n+-------+-----------+------------------+----------+------+\n|  10290|       9914|  8880.75494027386|        CA|  null|\n|  10298|       8834| 7840.286338156283|        TX|  null|\n|  10306|       8101| 6841.340007496114|        CA|  null|\n|  10314|       5393| 3764.861479692633|        CA|  null|\n|  10322|       9551|  9240.66060107629|        CA|  null|\n|  10330|       6426| 6027.026835409717|        WA|  null|\n|  10338|       7277| 6070.262206285235|        NY|  null|\n|  10490|       6438| 5408.020249899859|        CA|  null|\n|  10498|       6876| 6263.333658957699|        WA|  null|\n|  10506|       5181|3963.9419934482685|        NY|  null|\n|  10514|       6195| 5870.490200347467|        TX|  null|\n|  10522|       6732| 5902.880307462751|        TX|  null|\n|  10530|       9103|  7249.91336113477|        TX|  null|\n|  10538|       9837| 7909.667397548376|        NY|  null|\n|  10090|       9437| 7718.042681708777|        WA|  null|\n|  10098|       8340| 6734.315270614741|        NY|  null|\n|  10106|       5147|3645.8940584212623|        WA|  null|\n|  10114|       8404| 6520.506410364269|        CA|  null|\n|  10122|       6797| 5204.892327919268|        WA|  null|\n|  10130|       9123| 7808.341698226666|        TX|  null|\n+-------+-----------+------------------+----------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59ca8346-fc2f-4296-8ef8-99c8c027d9ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##  ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Transforming existing data\n",
    "\n",
    "\n",
    "Let's look into how we can transform existing data. But first, let's refine the view on the table because the schema has changed and the view needs to redefined to pick up the new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c687ef-2d2d-4c04-8e6f-78d4b6dd793f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined view 'loans_delta'\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\n",
    "print(\"Defined view 'loans_delta'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "567a78a0-02fc-435f-bedd-793ef5716047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Updating loan data to fix errors\n",
    "* Upon reviewing the data, we realized that all of the loans assigned to `addr_state = 'OR'` should have been assigned to `addr_state = 'WA'`.\n",
    "* In Parquet, to do an `update`, you would need to \n",
    "  * Copy all of the rows that are not affected into a new table\n",
    "  * Copy all of the rows that are affected into a DataFrame, perform the data modification\n",
    "  * Insert the previously noted DataFrame's rows into the new table\n",
    "  * Remove the old table\n",
    "  * Rename the new table to the old table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b2cba3-af67-4024-9a81-4bc746d56146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|addr_state|count(1)|\n+----------+--------+\n|        CA|    2253|\n|        WA|     605|\n|        TX|    1514|\n|        NY|    1502|\n|        OR|     178|\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef630870-57c0-436a-a75e-c696aec7b794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's fix the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a187b8-ec87-41fd-a18e-17853705ad52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.update(\"addr_state = 'OR'\",  {\"addr_state\": \"'WA'\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2b303b9-3af8-47a6-9888-3d0eca946c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see the data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca544e5-71a1-47f1-9242-808dced5aaf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n|addr_state|count(1)|\n+----------+--------+\n|        CA|    2253|\n|        WA|     783|\n|        TX|    1514|\n|        NY|    1502|\n+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e40a246-6092-4277-8236-3908363d36be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Deleting user-related data from a table General Data Protection Regulation (GDPR)\n",
    "\n",
    "You can remove data that matches a predicate from a Delta Lake table. Let's say we want to remove all the fully paid loans. Let's first see how many are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56739d3d-de35-4d5c-914d-d00ef6c5c288",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|    5134|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69c3786b-9e90-4840-89a0-2da3c62c2586",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now let's delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33c32141-b10e-42be-922a-6bbf30c32e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.delete(\"funded_amnt = paid_amnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1b68768-3853-4b0d-9a6b-02534f328863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's check the number of fully paid loans once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d1a2d9-d211-4d52-a558-66f2c6d03017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       0|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c011fbb4-d7b5-4398-8554-e9b145b3971f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Upserting change data to a table using merge\n",
    "A common use cases is Change Data Capture (CDC), where you have to replicate row changes made in an OLTP table to another table for OLAP workloads. To continue with our loan data example, say we have another table of new loan information, some of which are new loans and others are updates to existing loans. In addition, let’s say this changes table has the same schema as the loan_delta table. You can upsert these changes into the table using the DeltaTable.merge() operation which is based on the MERGE SQL command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158d2954-aea5-4f58-afc1-3cb7ce57ad1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|     11|       1000|   400.61|        NY|  null|\n|     21|       1000|    66.39|        NY|  null|\n|     28|       1200|    84.45|        NY|  null|\n+-------+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "113ccd07-012f-41b4-9c1f-ead32ad95238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's say we have some changes to this data, one loan has been paid off, and another new loan has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb48d44c-d97d-46db-885b-15b755c041ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"closed\"]\n",
    "\n",
    "items = [\n",
    "  (11, 1000, 1000.0, 'NY', True),   # loan paid off\n",
    "  (12, 1000, 0.0, 'NY', False)      # new loan\n",
    "]\n",
    "\n",
    "loanUpdates = spark.createDataFrame(items, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18ef2e4b-aaf9-49a9-bc80-90799f7ccbcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, let's update the table with the change data using the `merge` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd9b4e3e-5710-4720-91d3-abb4f0423c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "(deltaTable\n",
    "  .alias(\"t\")\n",
    "  .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n",
    "  .whenMatchedUpdateAll() \n",
    "  .whenNotMatchedInsertAll() \n",
    "  .execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f54ddf32-7c9c-4e7b-b1bd-63e156ebd811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see whether the table has been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d04a8d6c-c278-42ca-a285-27521a92ed58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|     21|       1000|    66.39|        NY|  null|\n|     28|       1200|    84.45|        NY|  null|\n|     11|       1000|   1000.0|        NY|  true|\n|     12|       1000|      0.0|        NY| false|\n+-------+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7d98f0e4-4621-46b0-9838-8cf800b92700",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Deduplicating data while inserting using insert-only merge\n",
    "\n",
    "The merge operation in Delta Lake supports an extended syntax beyond that specified by the ANSI standard. It supports advanced features like the following. \n",
    "- Delete actions: For example, `MERGE … WHEN MATCHED THEN DELETE`\n",
    "- Clause conditions: For example, `MERGE … WHEN MATCHED AND <condition> THEN ...``\n",
    "- Optional actions: All the MATCHED and NOT MATCHED clauses are optional.\n",
    "- Star syntax: For example, `UPDATE *` and `INSERT *` to update/insert all the columns in the target table with matching columns from the source dataset. The equivalent API in DeltaTable is `updateAll()` and `insertAll()`, which we have already seen.\n",
    "\n",
    "This allows you to express many more complex use cases with little code. For example, say you want to backfill the loan_delta table with historical data of past loans. But some of the historical data may already have been inserted in the table and you don't want to update them (since their emails may already have been updated). You can deduplicate by the loan_id while inserting by running the following merge operation with only the INSERT action (since the UPDATE action is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75cd245-256b-4367-93d7-c3e20dcb7a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|     21|       1000|    66.39|        NY|  null|\n|     28|       1200|    84.45|        NY|  null|\n|     11|       1000|   1000.0|        NY|  true|\n|     12|       1000|      0.0|        NY| false|\n+-------+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f15089b6-1bf6-4bca-b6bd-168645308522",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's say we have some historical data that we want to merge with this table. One of the historical loan exists in the current table but the historical table has old values, therefore it should not update the current value present in the table. And another historical does not exist in the current table, therefore it should be inserted into the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0aedd2-cb01-4f2c-a1de-0d07589e5aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cols = [\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\", \"closed\"]\n",
    "\n",
    "items = [\n",
    "  (11, 1000, 0.0, \"NY\", False),\n",
    "  (-100, 1000, 10.0, \"NY\", False)\n",
    "]\n",
    "\n",
    "historicalUpdates = spark.createDataFrame(items, cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7180ba15-76a7-4419-ad45-74bdf3facc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's do the merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f34b60a-7d7f-4980-9b24-c827e0eec81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "\n",
    "(deltaTable\n",
    "  .alias(\"t\")\n",
    "  .merge(historicalUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n",
    "  .whenNotMatchedInsertAll() \n",
    "  .execute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2eeff1e-e51b-46e7-b63a-90df16ba562a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see whether the table has been updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30fb3df0-c449-41af-93a4-28f581666a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+---------+----------+------+\n|loan_id|funded_amnt|paid_amnt|addr_state|closed|\n+-------+-----------+---------+----------+------+\n|     21|       1000|    66.39|        NY|  null|\n|     28|       1200|    84.45|        NY|  null|\n|     11|       1000|   1000.0|        NY|  true|\n|     12|       1000|      0.0|        NY| false|\n|   -100|       1000|     10.0|        NY| false|\n+-------+-----------+---------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fd3cd4c-10b0-4aa2-979a-9193b970ca68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notice that the only change in the table is that insert of new loan, and existing loans were not updated to old values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67a615be-3d56-4426-8e21-a5d9053ea742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Auditing data changes with operation history\n",
    "\n",
    "All changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b220d95b-0706-4f8c-a802-69daad8fc100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+--------------------+----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|       operation| operationParameters| job|         notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|     26|2025-06-04 21:37:39|6036967330759509|khodosevich.leoni...|           MERGE|{predicate -> [\"(...|null|{884566190556006}|0604-212634-gaq21xfo|         25|WriteSerializable|        false|{numTargetRowsCop...|        null|Databricks-Runtim...|\n|     25|2025-06-04 21:37:32|6036967330759509|khodosevich.leoni...|        OPTIMIZE|{predicate -> [],...|null|{884566190556006}|0604-212634-gaq21xfo|         24|SnapshotIsolation|        false|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     24|2025-06-04 21:37:06|6036967330759509|khodosevich.leoni...|           MERGE|{predicate -> [\"(...|null|{884566190556006}|0604-212634-gaq21xfo|         23|WriteSerializable|        false|{numTargetRowsCop...|        null|Databricks-Runtim...|\n|     23|2025-06-04 21:36:39|6036967330759509|khodosevich.leoni...|          DELETE|{predicate -> [\"(...|null|{884566190556006}|0604-212634-gaq21xfo|         22|WriteSerializable|        false|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     22|2025-06-04 21:36:05|6036967330759509|khodosevich.leoni...|          UPDATE|{predicate -> [\"(...|null|{884566190556006}|0604-212634-gaq21xfo|         21|WriteSerializable|        false|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     21|2025-06-04 21:35:15|6036967330759509|khodosevich.leoni...|           WRITE|{mode -> Append, ...|null|{884566190556006}|0604-212634-gaq21xfo|         20|WriteSerializable|         true|{numFiles -> 2, n...|        null|Databricks-Runtim...|\n|     20|2025-06-04 21:34:42|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         19|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     19|2025-06-04 21:34:32|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         18|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     18|2025-06-04 21:34:22|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         17|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     17|2025-06-04 21:34:12|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         16|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     16|2025-06-04 21:34:02|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         15|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     15|2025-06-04 21:33:52|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         14|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     14|2025-06-04 21:33:42|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         13|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     13|2025-06-04 21:33:32|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         12|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     12|2025-06-04 21:33:22|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         11|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     11|2025-06-04 21:33:13|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|         10|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|     10|2025-06-04 21:33:02|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|          9|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|      9|2025-06-04 21:32:52|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|          8|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|      8|2025-06-04 21:32:42|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|          7|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n|      7|2025-06-04 21:32:32|6036967330759509|khodosevich.leoni...|STREAMING UPDATE|{outputMode -> Ap...|null|{884566190556006}|0604-212634-gaq21xfo|          6|WriteSerializable|         true|{numRemovedFiles ...|        null|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+----------------+--------------------+----+-----------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, deltaPath)\n",
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a70ac3ab-cd4f-4693-9181-d9c6ea56e197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|version|timestamp          |operation|operationParameters                                                                                                                                                                        |\n+-------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|26     |2025-06-04 21:37:39|MERGE    |{predicate -> [\"(loan_id#19968L = loan_id#19948L)\"], matchedPredicates -> [], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}                       |\n|25     |2025-06-04 21:37:32|OPTIMIZE |{predicate -> [], zOrderBy -> [], batchId -> 0, auto -> true}                                                                                                                              |\n|24     |2025-06-04 21:37:06|MERGE    |{predicate -> [\"(loan_id#16514L = loan_id#16494L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|\n|23     |2025-06-04 21:36:39|DELETE   |{predicate -> [\"(cast(funded_amnt#15044 as double) = paid_amnt#15045)\"]}                                                                                                                   |\n+-------+-------------------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history(4).select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf9ee573-9d28-45a9-b4bb-91ef600239d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Querying previous snapshots of the table with time travel\n",
    "\n",
    "Delta Lake’s time travel feature allows you to access previous versions of the table. Here are some possible uses of this feature:\n",
    "\n",
    "* Auditing Data Changes\n",
    "* Reproducing experiments & reports\n",
    "* Rollbacks\n",
    "\n",
    "You can query by using either a timestamp or a version number using Python, Scala, and/or SQL syntax. For this examples we will query a specific version using the Python syntax.  \n",
    "\n",
    "For more information, refer to [Introducing Delta Time Travel for Large Scale Data Lakes](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html) and the [docs](https://docs.delta.io/latest/delta-batch.html#deltatimetravel).\n",
    "\n",
    "**Let's query the table's state before we deleted the data, which still contains the fully paid loans.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34c65a1-76b7-4bad-9e9d-d3303f6716f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|count(1)|\n+--------+\n|       0|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "previousVersion = deltaTable.history(1).select(\"version\").first()[0] - 3\n",
    "\n",
    "(spark.read.format(\"delta\")\n",
    "  .option(\"versionAsOf\", previousVersion)\n",
    "  .load(deltaPath)\n",
    "  .createOrReplaceTempView(\"loans_delta_pre_delete\"))\n",
    "\n",
    "spark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b46fdf7d-6ae6-4e0f-8dec-0a974a568303",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We see the same number of fully paid loans that we had seen before delete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ea35e3-e4e7-41ac-91a7-148e0c4a5ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "9-1 Delta Lake",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}